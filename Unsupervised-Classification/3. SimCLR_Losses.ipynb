{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "laughing-planner",
   "metadata": {},
   "source": [
    "### Set GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fossil-security",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-estate",
   "metadata": {},
   "source": [
    "### Set Dataset Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "located-tractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name = 'CIFAR10'\n",
    "# dataset_name = 'CIFAR100'\n",
    "# dataset_name = 'MNIST'\n",
    "# dataset_name = 'TINYIMAGENET'\n",
    "dataset_name = 'IMBALANCED_CIFAR10'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-request",
   "metadata": {},
   "source": [
    "### Run All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "supreme-belly",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models.resnet_cifar import resnet18\n",
    "from utils.config import create_config\n",
    "from utils.common_config import get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "official-documentation",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../results/imbalanced-cifar-10/pretext/model.pth.tar'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m     cfg_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfigs/CIFAR10_RESNET18.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     23\u001b[0m path_to_model \u001b[38;5;241m=\u001b[39m output_folder \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpretext/model.pth.tar\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 25\u001b[0m temp \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_to_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/lib/python3.10/site-packages/torch/serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/work/lib/python3.10/site-packages/torch/serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/work/lib/python3.10/site-packages/torch/serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../results/imbalanced-cifar-10/pretext/model.pth.tar'"
     ]
    }
   ],
   "source": [
    "output_folder = '../results/'\n",
    "if dataset_name == \"CIFAR10\":\n",
    "    output_folder += 'cifar-10/'\n",
    "    config_exp_path = './configs/pretext/simclr_cifar10.yml'\n",
    "    cfg_path = 'configs/CIFAR10_RESNET18.yaml'\n",
    "elif dataset_name == \"CIFAR100\":\n",
    "    output_folder += 'cifar-20/'\n",
    "    config_exp_path = './configs/pretext/simclr_cifar20.yml'\n",
    "    cfg_path = 'configs/CIFAR100_RESNET18.yaml'\n",
    "elif dataset_name == \"MNIST\":\n",
    "    output_folder += 'mnist/'\n",
    "    config_exp_path = './configs/pretext/simclr_mnist.yml'\n",
    "    cfg_path = 'configs/MNIST_RESNET18.yaml'\n",
    "elif dataset_name == \"TINYIMAGENET\":\n",
    "    output_folder += 'tinyimagenet/'\n",
    "    config_exp_path = './configs/pretext/simclr_tinyimagenet.yml'\n",
    "    cfg_path = 'configs/TINYIMAGENET_RESNET18.yaml'\n",
    "elif dataset_name == 'IMBALANCED_CIFAR10':\n",
    "    output_folder += 'imbalanced-cifar-10/'\n",
    "    config_exp_path = './configs/pretext/simclr_cifar10_im.yml'\n",
    "    cfg_path = 'configs/CIFAR10_RESNET18.yaml'\n",
    "    \n",
    "path_to_model = output_folder + 'pretext/model.pth.tar'\n",
    "\n",
    "temp = torch.load(path_to_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-flush",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "config_env_path = './configs/env.yml'\n",
    "p = create_config(config_env_path, config_exp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-dairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(p)\n",
    "model.load_state_dict(temp)\n",
    "model.eval()\n",
    "model.cuda();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-caribbean",
   "metadata": {},
   "source": [
    "### Creating a SimCLR Augmentation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-statement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Owner of this file: Thalles Silva\n",
    "# Source: https://github.com/sthalles/PyTorch-BYOL\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class GaussianBlur(object):\n",
    "    \"\"\"Blurs a single image on CPU\"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size):\n",
    "        radias = kernel_size // 2\n",
    "        kernel_size = radias * 2 + 1\n",
    "        self.blur_h = nn.Conv2d(3, 3, kernel_size=(kernel_size, 1),\n",
    "                                stride=1, padding=0, bias=False, groups=3)\n",
    "        self.blur_v = nn.Conv2d(3, 3, kernel_size=(1, kernel_size),\n",
    "                                stride=1, padding=0, bias=False, groups=3)\n",
    "        self.k = kernel_size\n",
    "        self.r = radias\n",
    "\n",
    "        self.blur = nn.Sequential(\n",
    "            nn.ReflectionPad2d(radias),\n",
    "            self.blur_h,\n",
    "            self.blur_v\n",
    "        )\n",
    "\n",
    "        self.pil_to_tensor = transforms.ToTensor()\n",
    "        self.tensor_to_pil = transforms.ToPILImage()\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = self.pil_to_tensor(img).unsqueeze(0)\n",
    "\n",
    "        sigma = np.random.uniform(0.1, 2.0)\n",
    "        x = np.arange(-self.r, self.r + 1)\n",
    "        x = np.exp(-np.power(x, 2) / (2 * sigma * sigma))\n",
    "        x = x / x.sum()\n",
    "        x = torch.from_numpy(x).view(1, -1).repeat(3, 1)\n",
    "\n",
    "        self.blur_h.weight.data.copy_(x.view(3, 1, self.k, 1))\n",
    "        self.blur_v.weight.data.copy_(x.view(3, 1, 1, self.k))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            img = self.blur(img)\n",
    "            img = img.squeeze()\n",
    "\n",
    "        img = self.tensor_to_pil(img)\n",
    "\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latter-intent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_blur(im, input_shape):\n",
    "    \"\"\"Applies GaussianBlur to input image.\"\"\"\n",
    "    blur = GaussianBlur(kernel_size=int(0.1 * input_shape))\n",
    "    if dataset_name == 'MNIST':\n",
    "        return Image.fromarray(np.uint8(im)).filter(ImageFilter.GaussianBlur(radius = 3)).convert(\"L\")\n",
    "    return Image.fromarray(np.uint8(blur(im))).convert(\"RGB\")\n",
    "\n",
    "\n",
    "def horizontal_flip(im):\n",
    "    \"\"\"Performs horizontal flip.\"\"\"\n",
    "    if dataset_name == 'MNIST':\n",
    "        return Image.fromarray(np.uint8(im)).transpose(PIL.Image.FLIP_LEFT_RIGHT).convert(\"L\")\n",
    "    return Image.fromarray(np.uint8(im[:, ::-1, :])).convert(\"RGB\")\n",
    "\n",
    "def color_jitter(im):\n",
    "    \"\"\"Performs Color Jitter based on SimCLR paper.\"\"\"\n",
    "    s=1\n",
    "    im = Image.fromarray(np.uint8(im)).convert(\"RGB\")\n",
    "    color_jitter = transforms.ColorJitter(0.8 * s, 0.8 * s, 0.8 * s, 0.2 * s)\n",
    "    out = Image.fromarray(np.uint8(color_jitter(im)))\n",
    "    if dataset_name == 'MNIST':\n",
    "        return out.convert(\"L\")\n",
    "    return out.convert(\"RGB\")\n",
    "\n",
    "def gray_scale(im):\n",
    "    \"\"\"Converts the input image into a grayscale image.\"\"\"\n",
    "    gray = transforms.RandomGrayscale(p=1)\n",
    "    im = Image.fromarray(np.uint8(im)).convert(\"RGB\")\n",
    "    if dataset_name == 'MNIST':\n",
    "        return im.convert(\"L\")\n",
    "    return im.convert(\"L\").convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-asbestos",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from typing import Any, Tuple\n",
    "from PIL import Image, ImageFilter \n",
    "import PIL\n",
    "import torchvision\n",
    "\n",
    "\n",
    "class SimCLRAugmentedDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Gives a dataset with all SimCLR Augmentations for each image. \n",
    "    \n",
    "    Args:\n",
    "        name (string): Name of the dataset. E.g., 'CIFAR10', 'TINYIMAGENET', etc.\n",
    "        dataset (Dataset, optional): PyTorch Dataset object. \n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, name, dataset):\n",
    "        super(SimCLRAugmentedDataset, self).__init__()\n",
    "        self.name = name\n",
    "        self.dataset = dataset\n",
    "        if self.name in ['CIFAR10', 'CIFAR100', 'MNIST', 'SVHN', 'IMBALANCED_CIFAR10', 'IMBALANCED_CIFAR100']:\n",
    "            self.old_samples = self.dataset.data\n",
    "            self.labels = []\n",
    "        else: # Tiny ImageNet\n",
    "            self.old_samples = [item[0] for item in self.dataset.samples]\n",
    "            self.old_samples = [np.asarray(Image.open(img).convert(\"L\")) for img in self.old_samples]\n",
    "        self.data, self.targets = self.create_augmentations()\n",
    "        self.transform = self.dataset.transform\n",
    "        \n",
    "    def create_augmentations(self):\n",
    "        imgs, labels = [], []\n",
    "        for idx in tqdm(range(len(self.old_samples)), desc=\"Creating Augmented Dataset\"):\n",
    "            img = self.old_samples[idx]\n",
    "            augmented_imgs = [\n",
    "                torchvision.transforms.ToPILImage()(img),\n",
    "                horizontal_flip(img),\n",
    "                gaussian_blur(img, img.shape[0]),\n",
    "                color_jitter(img),\n",
    "                gray_scale(img)\n",
    "            ]\n",
    "            augmented_labels = torch.LongTensor([self.dataset.targets[idx] for x in range(5)])\n",
    "            if self.name == 'MNIST':\n",
    "                imgs.extend(augmented_imgs[:-1])\n",
    "                labels.extend(augmented_labels[:-1])\n",
    "            else:\n",
    "                imgs.extend(augmented_imgs)\n",
    "                labels.extend(augmented_labels)\n",
    "        return imgs, labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        head = \"SimCLR Augmentated Dataset of \" + self.name \n",
    "        body = [\"Augmentations: GaussianBlur, ColorJitter, HorizontalFlip, GrayScale.\"]\n",
    "        body += [\"Number of datapoints (5 x original dataset): {}\".format(self.__len__())]\n",
    "        body += [\"Normalization Values:\"]\n",
    "        if self.transform is not None:\n",
    "            body += [repr(self.transform)]\n",
    "        lines = [head] + [\" \" + line for line in body]\n",
    "        return '\\n'.join(lines)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "#         print(img.shape)\n",
    "#         print(type(img))\n",
    "#         img = torchvision.transforms.ToPILImage()(img)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-technology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating Augmented Dataset:   0%|          | 29/13996 [00:00<00:49, 281.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Mode: Contain 13996 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating Augmented Dataset: 100%|██████████| 13996/13996 [00:36<00:00, 383.30it/s]\n"
     ]
    }
   ],
   "source": [
    "from pycls.datasets.data import Data\n",
    "from pycls.config import cfg\n",
    "\n",
    "\n",
    "cfg.merge_from_file(cfg_path)\n",
    "cfg.DATASET.NAME = dataset_name\n",
    "data_obj = Data(cfg)\n",
    "data_obj.eval_mode = True\n",
    "train_data, train_size = data_obj.getDataset(save_dir=f'../{cfg.DATASET.ROOT_DIR}', isTrain=True, isDownload=True)\n",
    "simclr_data = SimCLRAugmentedDataset(dataset_name, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-dover",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimCLR Augmentated Dataset of IMBALANCED_CIFAR10\n",
       " Augmentations: GaussianBlur, ColorJitter, HorizontalFlip, GrayScale.\n",
       " Number of datapoints (5 x original dataset): 69980\n",
       " Normalization Values:\n",
       " Compose(\n",
       "    RandomCrop(size=(32, 32), padding=4)\n",
       "    ToTensor()\n",
       "    Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.247, 0.2435, 0.2616])\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simclr_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-growth",
   "metadata": {},
   "source": [
    "### DO NOT Change Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-roots",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == 'MNIST':\n",
    "    batch_size = 4\n",
    "else:\n",
    "    batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-graphic",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet = [i for i in range(len(simclr_data))]\n",
    "trainSet = np.array(trainSet, dtype=np.ndarray)\n",
    "train_dataloader = data_obj.getSequentialDataLoader(indexes=trainSet, batch_size=5, data=simclr_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-warner",
   "metadata": {},
   "source": [
    "### SimCLR Augmentations Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-supplier",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    plt.figure(figsize=(10,3))\n",
    "    img = img/2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.savefig('SimCLR_Loss_Example.png')\n",
    "    plt.show()\n",
    "    \n",
    "dataiter = iter(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-tracker",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "images, labels = dataiter.next()\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-repair",
   "metadata": {},
   "source": [
    "### Calculating SimCLRLoss between image and its augmented variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-fourth",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating SimCLR Loss: 100%|██████████| 13996/13996 [03:37<00:00, 64.44it/s]\n"
     ]
    }
   ],
   "source": [
    "from losses.losses import SimCLRLoss\n",
    "\n",
    "image_losses = []\n",
    "for images, lbl in tqdm(train_dataloader, desc=\"Calculating SimCLR Loss: \"):\n",
    "    b, c, h, w = images.size()\n",
    "    criterion = SimCLRLoss(temperature=0.1)\n",
    "    all_losses = []\n",
    "    with torch.no_grad():\n",
    "        for idx in range(1, b):\n",
    "            input_ = torch.cat([images[0],images[idx]])\n",
    "            input_ = input_.view(-1, c, h, w) \n",
    "            input_ = input_.cuda(non_blocking=True)\n",
    "            output = model(input_).view(1, 2, -1)\n",
    "            loss = criterion(output)\n",
    "            all_losses.append(loss.item())\n",
    "        image_losses.append(np.mean(all_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-talent",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_losses=np.array(image_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-opposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'{output_folder}/{dataset_name}_SimCLR_losses.npy', image_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

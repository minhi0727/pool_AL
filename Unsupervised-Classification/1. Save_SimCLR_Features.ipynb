{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "vulnerable-analysis",
   "metadata": {},
   "source": [
    "### Set GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "controversial-spank",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"2\"\n",
    "##아직활용 X "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprised-design",
   "metadata": {},
   "source": [
    "## Set Dataset Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acknowledged-graduation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name = 'CIFAR10'\n",
    "# dataset_name = 'TINYIMAGENET'\n",
    "# dataset_name = 'MNIST'\n",
    "# dataset_name = 'TINYIMAGENET'\n",
    "dataset_name = 'IMBALANCED_CIFAR10'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-surgeon",
   "metadata": {},
   "source": [
    "### Run All Now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sticky-drinking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models.resnet_stl import resnet18\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def add_path(path):\n",
    "    if path not in sys.path:\n",
    "        sys.path.insert(0, path)\n",
    "\n",
    "add_path(os.path.abspath('/home/chominhi/work/init-pools-dal-main/Unsupervised-Classification'))\n",
    "\n",
    "\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from models.resnet_cifar import resnet18\n",
    "from utils.memory import MemoryBank\n",
    "from utils.train_utils import simclr_train\n",
    "from utils.utils import fill_memory_bank\n",
    "from utils.config import create_config\n",
    "from utils.common_config import get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "banned-narrative",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m output_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../results/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdataset_name\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCIFAR10\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      3\u001b[0m     output_folder \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcifar-10/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m     config_exp_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./configs/pretext/simclr_cifar10.yml\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset_name' is not defined"
     ]
    }
   ],
   "source": [
    "output_folder = '../results/'\n",
    "if dataset_name == \"CIFAR10\":\n",
    "    output_folder += 'cifar-10/'\n",
    "    config_exp_path = './configs/pretext/simclr_cifar10.yml'\n",
    "    cfg_path = 'configs/CIFAR10_RESNET18.yaml'\n",
    "elif dataset_name == \"CIFAR100\":\n",
    "    output_folder += 'cifar-20/'\n",
    "    config_exp_path = './configs/pretext/simclr_cifar20.yml'\n",
    "    cfg_path = 'configs/CIFAR100_RESNET18.yaml'\n",
    "elif dataset_name == \"MNIST\":\n",
    "    output_folder += 'mnist/'\n",
    "    config_exp_path = './configs/pretext/simclr_mnist.yml'\n",
    "    cfg_path = 'configs/MNIST_RESNET18.yaml'\n",
    "elif dataset_name == \"TINYIMAGENET\":\n",
    "    output_folder += 'tinyimagenet/'\n",
    "    config_exp_path = './configs/pretext/simclr_tinyimagenet.yml'\n",
    "    cfg_path = 'configs/TINYIMAGENET_RESNET18.yaml'\n",
    "elif dataset_name == 'IMBALANCED_CIFAR10':\n",
    "    output_folder += 'imbalanced-cifar-10/'\n",
    "    config_exp_path = './configs/pretext/simclr_cifar10_im.yml'\n",
    "    cfg_path = 'configs/CIFAR10_RESNET18.yaml'\n",
    "    \n",
    "path_to_model = output_folder + 'pretext/model.pth.tar'\n",
    "\n",
    "temp = torch.load(path_to_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-adobe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "config_env_path = './configs/env.yml'\n",
    "p = create_config(config_env_path, config_exp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-rugby",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(p)\n",
    "model.load_state_dict(temp)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-nigeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycls.datasets.data import Data\n",
    "from pycls.config import cfg\n",
    "cfg.merge_from_file(cfg_path)\n",
    "cfg.DATASET.NAME = dataset_name\n",
    "data_obj = Data(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-parcel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess Operations Selected ==>  [RandomCrop(size=(32, 32), padding=4), ToTensor(), Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.247, 0.2435, 0.2616])]\n",
      "Files already downloaded and verified\n",
      "Train Mode: Contain 13996 images\n"
     ]
    }
   ],
   "source": [
    "train_data, train_size = data_obj.getDataset(save_dir=f'../{cfg.DATASET.ROOT_DIR}', isTrain=True, isDownload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-mumbai",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainSet_path, valSet_path = data_obj.makeTVSets(val_split_ratio=cfg.DATASET.VAL_RATIO, data=train_data,\\\n",
    "#                                  seed_id=cfg.RNG_SEED, save_dir='exp')\n",
    "# trainSet, valSet = data_obj.loadTVPartitions(trainSetPath=trainSet_path, valSetPath=valSet_path)\n",
    "trainSet = [i for i in range(train_size)]\n",
    "trainSet = np.array(trainSet, dtype=np.ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-advertising",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_representation(clf_model, idx_set, dataset):\n",
    "    clf_model.cuda()\n",
    "    tempIdxSetLoader = data_obj.getSequentialDataLoader(indexes=idx_set, batch_size=int(cfg.TRAIN.BATCH_SIZE/cfg.NUM_GPUS), data=dataset)\n",
    "    features = []\n",
    "\n",
    "    print(f\"len(dataLoader): {len(tempIdxSetLoader)}\")\n",
    "\n",
    "    for i, (x, _) in enumerate(tqdm(tempIdxSetLoader, desc=\"Extracting Representations\")):\n",
    "        with torch.no_grad():\n",
    "            x = x.cuda()\n",
    "            x = x.type(torch.cuda.FloatTensor)\n",
    "            temp_z = clf_model(x)\n",
    "            features.append(temp_z.cpu().numpy())\n",
    "\n",
    "    features = np.concatenate(features, axis=0)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-contract",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Representations:   1%|          | 1/110 [00:00<00:21,  5.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(dataLoader): 110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Representations: 100%|██████████| 110/110 [00:05<00:00, 20.28it/s]\n"
     ]
    }
   ],
   "source": [
    "features = get_representation(model, trainSet, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-event",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13996, 128)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "still-toilet",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'{output_folder}/{dataset_name}_features{features.shape[1]}.npy', features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
